{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "- GitHub: https://github.com/hwchase17/langchain\n",
    "- Docs: https://python.langchain.com/en/latest/index.html\n",
    "\n",
    "### Overview:\n",
    "- Installation\n",
    "- LLMs\n",
    "- Prompt Templates\n",
    "- Chains\n",
    "- Agents and Tools\n",
    "- Memory\n",
    "- Document Loaders\n",
    "- Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Step 02: Setup the Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunomaciel/miniforge3/envs/langchain/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/brunomaciel/miniforge3/envs/langchain/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Comment vieux Ãªtes-vous?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# https://huggingface.co/google/flan-t5-large\n",
    "llm = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0, 'max_length':64})\n",
    "\n",
    "llm(\"translate english to french: How old are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunomaciel/miniforge3/envs/langchain/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese restaurant\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0, 'max_length':64})\n",
    "name = llm.predict('I want to open a restaurant for Chinese food. Suggest a fency name for this.')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Russian food. Suggest a fency name for this.\n",
      "Russian restaurant\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "p = prompt_template_name.format(cuisine=\"Russian\")\n",
    "print(p)\n",
    "print(llm.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 04: Prompt Templates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently in the above applications we are writing an entire prompt, if you are creating a user directed application then this is not an ideal case\n",
    "\n",
    "LangChain faciliates prompt management and optimization.\n",
    "\n",
    "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many Large Language Model applications we donot pass the user input directly to the Large Language Model, we add the user input to a large piece of text called prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunomaciel/miniforge3/envs/langchain/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "llm = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0, 'max_length':64})\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sock mania\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.run(\"colorful socks\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunomaciel/miniforge3/envs/langchain/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0, 'max_length':64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggeset a fency name for this.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mars Cafe\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "response = chain.run(\"martian\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Can we combine Multiple PromptTemplates, We will try to combine Multiple PromptTemplates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**The output from the first PromptTemplate is passed to the next PromptTemplate as input**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**To comine the Chain and  to set a sequence for that we use SimpleSequentialChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Simple Sequential Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunomaciel/miniforge3/envs/langchain/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0.9, 'max_length':64})\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain=LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "prompt_template_items=PromptTemplate(\n",
    "    input_variables=['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chorizo, chorizo enchilada, chorizo enchilada, chorizo enchilada, chorizo enchilada, chorizo enchilada, chorizo enchilada\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "\n",
    "content = chain.run(\"chilean\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**There is a issue with SimpleSequentialChain it only shows last input information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**To show the entire information i will use SequentialChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Sequential Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunomaciel/miniforge3/envs/langchain/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0.9, 'max_length':64})\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables=['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'pakistani',\n",
       " 'restaurant_name': 'Pakistani restaurant',\n",
       " 'menu_items': 'samosa'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({'cuisine': 'pakistani'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 06. Agents and Tools**\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
    "\n",
    "\n",
    "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
    "\n",
    "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
    "- LLM: The language model powering the agent.\n",
    "- Agent: The agent to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "- GitHub: https://github.com/hwchase17/langchain\n",
    "- Docs: https://python.langchain.com/en/latest/index.html\n",
    "\n",
    "### Overview:\n",
    "- Installation\n",
    "- LLMs\n",
    "- Prompt Templates\n",
    "- Chains\n",
    "- Agents and Tools\n",
    "- Memory\n",
    "- Document Loaders\n",
    "- Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Step 06. Agents and Tools**\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
    "\n",
    "\n",
    "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
    "\n",
    "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
    "- LLM: The language model powering the agent.\n",
    "- Agent: The agent to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent is a very powerful concept in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example I have to travel from Dubai to Canada, I type this in ChatGPT\n",
    "\n",
    "\n",
    "\n",
    "---> Give me  two flight options from Dubai to Canada on September 1, 2023 | ChatGPT will not be able to answer because has knowledge till\n",
    "September 2021\n",
    "\n",
    "\n",
    "\n",
    "ChatGPT plus has Expedia Plugin, if we enable this plugin it will go to Expedia Plugin and will try to pull information about Flights & it will show the information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**What exactly happens when we try to enable this plugin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we think about LLM. Many people think that it is just a knowledge engine, it has knowledge and it will try to give answer based on that knowledge but the knowledge is only limited to September 2021. The think that most people missout is that Large Lanaguage Model has a reasoning engine, and using that reasoning engine it can figure out when someone types this type of Question\n",
    "\n",
    "\n",
    "Give me  two flight options from Dubai to Canada on September 1, 2023\n",
    "\n",
    "\n",
    "As a human we go to Expedia as we have a reasoning engine in our brain.\n",
    "\n",
    "LLM has a reasoning engine as well, so it will figure out the Source, Destination, Date and it will call Expedia Plugin and it will return response back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**How much is US GDP in 2022? plus 5**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LLM has a reasoning engine to answer that question it will go to Google Search Tool, it will find that answer and then it will use Math Tool and do plus 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**An agent has access to a suite of tools, and determines which ones to use depending on the user input.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Agent will conenct with external tools and it will use LLM reasoning capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#All the tools like Google Search Tool and Math Tool are available as part of LangChain and you can configure  agent, so agent is nothing but using all these tools and LLM reasoning capabilities to perform a given task  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#To access Google Search Results in Real Time we use serpapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### serpapi and llm-math tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a text LLM, first try zero-shot-react-description, aka. the MRKL agent for LLMs.\n",
    "\n",
    "\n",
    "If you're using a Chat Model, try chat-zero-shot-react-description, aka. the MRKL agent for Chat Models.\n",
    "\n",
    "\n",
    "If you're using a Chat Model and want to use memory, try chat-conversational-react-description, the Conversational agent.\n",
    "\n",
    "\n",
    "If you have a complex task that requires many steps and you're interested in experimenting with a new type of agent, try the Plan-and-Execute agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SERPAPI_API_KEY'] = '13580935590d911db7aa931fad0d2628df5be2a843ae5a94bf906b1d3beecc81'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunomaciel/miniforge3/envs/langchain/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "llm = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0.9, 'max_length':64})\n",
    "#Google Search API\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools(['llm-math'], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `The GDP of US in 2022 plus 5 was . The final answer: .`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/agent.py:928\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 928\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    929\u001b[0m         intermediate_steps,\n\u001b[1;32m    930\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    931\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    932\u001b[0m     )\n\u001b[1;32m    933\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/agent.py:542\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 542\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(full_output)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/mrkl/output_parser.py:61\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAction\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*:[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*(.*?)\u001b[39m\u001b[39m\"\u001b[39m, text, re\u001b[39m.\u001b[39mDOTALL):\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     62\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: `\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m         observation\u001b[39m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[1;32m     64\u001b[0m         llm_output\u001b[39m=\u001b[39mtext,\n\u001b[1;32m     65\u001b[0m         send_to_llm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39msearch(\n\u001b[1;32m     68\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*Action\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*Input\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*:[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*(.*)\u001b[39m\u001b[39m\"\u001b[39m, text, re\u001b[39m.\u001b[39mDOTALL\n\u001b[1;32m     69\u001b[0m ):\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `The GDP of US in 2022 plus 5 was . The final answer: .`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/brunomaciel/Documents/git/LLM-study/main.ipynb CÃ©lula 54\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y250sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Let's test it out!\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y250sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWhat was the GDP of US in 2022 plus 5?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:503\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 503\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    504\u001b[0m         _output_key\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    509\u001b[0m         _output_key\n\u001b[1;32m    510\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:308\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    309\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    310\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:302\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    295\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    296\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    297\u001b[0m     inputs,\n\u001b[1;32m    298\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    303\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    304\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/agent.py:1141\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1141\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1142\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1143\u001b[0m         color_mapping,\n\u001b[1;32m   1144\u001b[0m         inputs,\n\u001b[1;32m   1145\u001b[0m         intermediate_steps,\n\u001b[1;32m   1146\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1147\u001b[0m     )\n\u001b[1;32m   1148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1149\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1150\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1151\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/agent.py:939\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    937\u001b[0m     raise_error \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 939\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    940\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn output parsing error occurred. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    941\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis is the error: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m     )\n\u001b[1;32m    945\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    946\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `The GDP of US in 2022 plus 5 was . The final answer: .`"
     ]
    }
   ],
   "source": [
    "# Let's test it out!\n",
    "agent.run(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Answer: 2001`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/agent.py:928\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 928\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    929\u001b[0m         intermediate_steps,\n\u001b[1;32m    930\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    931\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    932\u001b[0m     )\n\u001b[1;32m    933\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/agent.py:542\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39mpredict(callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 542\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_parser\u001b[39m.\u001b[39;49mparse(full_output)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/mrkl/output_parser.py:61\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAction\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*:[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*(.*?)\u001b[39m\u001b[39m\"\u001b[39m, text, re\u001b[39m.\u001b[39mDOTALL):\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     62\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not parse LLM output: `\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m         observation\u001b[39m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[1;32m     64\u001b[0m         llm_output\u001b[39m=\u001b[39mtext,\n\u001b[1;32m     65\u001b[0m         send_to_llm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m     )\n\u001b[1;32m     67\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39msearch(\n\u001b[1;32m     68\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*Action\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*Input\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms*:[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms]*(.*)\u001b[39m\u001b[39m\"\u001b[39m, text, re\u001b[39m.\u001b[39mDOTALL\n\u001b[1;32m     69\u001b[0m ):\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `Answer: 2001`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/brunomaciel/Documents/git/LLM-study/main.ipynb CÃ©lula 55\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y253sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y253sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     tools,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y253sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     llm,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y253sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y253sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y253sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y253sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Let's test it out!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brunomaciel/Documents/git/LLM-study/main.ipynb#Y253sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mIn what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:503\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    502\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 503\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    504\u001b[0m         _output_key\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    508\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    509\u001b[0m         _output_key\n\u001b[1;32m    510\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:308\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    309\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    310\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    311\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:302\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    295\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    296\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    297\u001b[0m     inputs,\n\u001b[1;32m    298\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    303\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    304\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/agent.py:1141\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1141\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1142\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1143\u001b[0m         color_mapping,\n\u001b[1;32m   1144\u001b[0m         inputs,\n\u001b[1;32m   1145\u001b[0m         intermediate_steps,\n\u001b[1;32m   1146\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1147\u001b[0m     )\n\u001b[1;32m   1148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1149\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1150\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1151\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/langchain/lib/python3.9/site-packages/langchain/agents/agent.py:939\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    937\u001b[0m     raise_error \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39mif\u001b[39;00m raise_error:\n\u001b[0;32m--> 939\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    940\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn output parsing error occurred. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    941\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis is the error: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m     )\n\u001b[1;32m    945\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    946\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Answer: 2001`"
     ]
    }
   ],
   "source": [
    "# install this package: pip install wikipedia\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
